{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Script for processing segmented sentences with spaCy\n",
        "# - Loads sentences with segments marked by <seg>\n",
        "# - Analyzes sentences with spaCy\n",
        "# - Builds a DataFrame with token-level features\n",
        "# - Saves \"clean\" sentences without <seg> tags to a separate file\n",
        "# ==========================================================\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "!python -m spacy download it_core_news_sm\n",
        "\n",
        "# Load the spaCy model for Italian\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "\n",
        "# List to collect token-level data\n",
        "tutti_dati = []\n",
        "\n",
        "# List to save clean sentences (without <seg> tags)\n",
        "frasi_segmentate = []\n",
        "\n",
        "# POS tags of interest\n",
        "pos_tags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"CCONJ\", \"DET\", \"NUM\", \"PUNCT\", \"PRON\", \"ADP\"]\n",
        "\n",
        "# Function to load numbered sentences from a text file\n",
        "# Each line of the input file should contain a sentence in the format: index<TAB>sentence\n",
        "\n",
        "def carica_frasi_numerate(file_path):\n",
        "    # Opens the file in read mode, using UTF-8 and replacing any faulty characters\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        righe = f.readlines()\n",
        "\n",
        "    frasi = []\n",
        "\n",
        "    # Iterate all lines in the file one by one\n",
        "    for i, riga in enumerate(righe):\n",
        "        riga = riga.strip()  # removes leading/trailing whitespace\n",
        "\n",
        "        # Skip lines that do not contain a tab (malformed lines)\n",
        "        if \"\\t\" not in riga:\n",
        "            print(f\"[LINE {i+1}] Ignored line (no tab): {riga!r}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Split the line into two parts: numeric index and sentence\n",
        "            idx, frase = riga.split(\"\\t\", 1)\n",
        "\n",
        "            # Append a tuple (integer index, sentence) to the list\n",
        "            frasi.append((int(idx), frase))\n",
        "        except Exception as e:\n",
        "            # Print error if something goes wrong during parsing\n",
        "            print(f\"[LINE {i+1}] Parsing error: {e}\")\n",
        "\n",
        "    return frasi  # Returns the list of tuples (index, sentence)\n",
        "\n",
        "# Specify the input file path\n",
        "file_input = \"/content/corpus_in_frasi_anfass.txt\"\n",
        "\n",
        "# Load numbered sentences from the file\n",
        "frasi_numerate = carica_frasi_numerate(file_input)\n",
        "\n",
        "# For loop over sentences with <seg> tags\n",
        "for frase_idx, frase_con_seg in frasi_numerate:\n",
        "    # Split the sentence into segments, remove unnecessary spaces\n",
        "    segmenti = [s.strip() for s in frase_con_seg.split(\"<seg>\")]\n",
        "\n",
        "    # Rebuild the sentence without <seg>\n",
        "    frase_pulita = \" \".join(segmenti)\n",
        "\n",
        "    # Apply spaCy to the complete sentence\n",
        "    doc = nlp(frase_pulita)\n",
        "\n",
        "    # Save the clean sentence with its index\n",
        "    frasi_segmentate.append((frase_idx, frase_pulita))\n",
        "\n",
        "    # Tokenize each segment separately\n",
        "    segmenti_tokenizzati = [nlp(seg) for seg in segmenti]\n",
        "\n",
        "    # Calculate lengths (in tokens) of each segment\n",
        "    lunghezze = [len(seg) for seg in segmenti_tokenizzati]\n",
        "\n",
        "    # Calculate boundaries between segments\n",
        "    confini = set()\n",
        "    offset = 0\n",
        "    for lung in lunghezze[:-1]:  # exclude the last segment\n",
        "        offset += lung\n",
        "        confini.add(offset - 1)  # last token of the segment\n",
        "\n",
        "    # Iterate over tokens in the complete sentence\n",
        "    for i_token, token in enumerate(doc):\n",
        "        if token.text.strip() == \"\":\n",
        "            continue  # Skip empty/space tokens\n",
        "\n",
        "        token_text = token.text\n",
        "        pos = token.pos_\n",
        "\n",
        "        # Normalize some POS categories\n",
        "        if pos == \"AUX\":\n",
        "            pos = \"VERB\"\n",
        "        elif pos == \"SCONJ\":\n",
        "            pos = \"CCONJ\"\n",
        "        elif pos not in pos_tags:\n",
        "            pos = \"OTHER\"\n",
        "\n",
        "        # Indicate if the token is at the end of a segment\n",
        "        segmenta = 1 if i_token in confini else 0\n",
        "\n",
        "        # Save relevant information into a list of dictionaries\n",
        "        tutti_dati.append({\n",
        "            \"token\": token_text,\n",
        "            \"segmenta\": segmenta,\n",
        "            \"frase_idx\": frase_idx,\n",
        "            \"frase_len_token\": len(doc),\n",
        "            \"frase_len_char\": len(frase_pulita),\n",
        "            \"token_len_char\": len(token_text),\n",
        "            \"distanza_da_prima_parola\": i_token,\n",
        "            \"pos\": pos\n",
        "        })\n",
        "\n",
        "# Create a DataFrame with all token-level data\n",
        "df = pd.DataFrame(tutti_dati)\n",
        "\n",
        "# Create binary columns for each POS tag\n",
        "for pos_tag in pos_tags:\n",
        "    df[pos_tag] = df['pos'].apply(lambda x: 1 if x == pos_tag else 0)\n",
        "\n",
        "# Reorder the column order in the DataFrame\n",
        "cols_pos = pos_tags\n",
        "other_cols = [col for col in df.columns if col not in cols_pos + [\"frase_idx\", \"token\", \"pos\"]]\n",
        "df = df[[\"frase_idx\", \"token\"] + cols_pos + other_cols]\n",
        "\n",
        "# Save the clean sentences (without <seg>) to a file\n",
        "with open(\"frasi.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, frase in frasi_segmentate:\n",
        "        f.write(f\"{idx}\\t{frase}\\n\")\n",
        "\n",
        "# Download the file\n",
        "from google.colab import files\n",
        "files.download(\"frasi.txt\")\n",
        "\n",
        "\n",
        "# Save the dataframe to a pickle file\n",
        "import pickle\n",
        "\n",
        "# Save the dataframe to a pickle file (automatically creates the file if it doesn't exist)\n",
        "with open('anfass.pkl', 'wb') as file:\n",
        "    pickle.dump(df, file)  # save (dump) the dataframe df into the opened file\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Path of the file to download\n",
        "file_path = \"/content/anfass.pkl\"\n",
        "\n",
        "# Download the file\n",
        "files.download(file_path)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Save the DataFrame as CSV\n",
        "df.to_csv('anfass.csv', index=False)  # 'index=False' avoids saving the index as a column in the CSV\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Path of the file you want to download\n",
        "file_path_2 = \"/content/anfass.csv\"\n",
        "\n",
        "# Download the file\n",
        "files.download(file_path_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "4wHf0cZ1dzVO",
        "outputId": "580cff0f-74a9-42c0-b080-1f074dba045d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_122352d2-8c82-4fa7-9c56-f6051e30256f\", \"frasi.txt\", 30340)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_911a688c-72a7-4df3-b1c7-772065cee52f\", \"anfass.pkl\", 729246)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_89b16da7-99f6-473f-af61-c56c5b7c8ed1\", \"anfass.csv\", 224603)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}