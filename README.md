Sentence Segmentation Project
This repository contains scripts and notebooks for sentence segmentation tasks, focusing on training and evaluating a Decision Tree classifier and comparing it with large language models (LLMs).

Overview
The notebooks are ready to run on Google Colab with Google Drive mounted.

The scripts can be run locally or in any Python environment with the required packages.

How to Run the Project
Running the scripts (local or remote environment)
To reproduce the full pipeline, run the scripts in the following order:

Preprocessing script
Prepare and clean the corpus files, generate the DataFrame for model training.

Training script
Train the Decision Tree classifier on the preprocessed DataFrame.

Evaluation script
Evaluate the trained model on validation and test sets, generate metrics and confusion matrices.

Test set extraction script
Extract sentences from the corpus corresponding to the test set indices for later evaluation of the LLM model.

Make sure all necessary input files are in the expected directories before running each script.

Running the notebooks on Google Colab
Mount your Google Drive in the notebook:

python
Copia
Modifica
from google.colab import drive
drive.mount('/content/drive')
Data files:
Before running a notebook, upload the required input files (found in the Data/ folder of this repo) into the /content directory of your Colab session.
This ensures the notebook can access the data correctly.

Output files generated by notebooks will be saved in /content by default; remember to move them to Drive if you want to keep them.

Run the notebooks in the recommended order that matches the script pipeline (preprocessing → training → evaluation → test extraction).

Manual Files
Several important files were created manually or semi-manually (i.e., with some automated methods plus manual revision) and are not generated by scripts in this repository. They are necessary for training and evaluation:

output_corpus_no_ricette.txt: Cleaned corpus without recipe-like sections, used to build the training DataFrame.

corpus_in_frasi_anfass_tagliato_perdecision.txt: Derived corpus excluding sentences modified by the LLM model, to ensure fair evaluation comparability.

Final evaluation files: frasi_semplificate_gemma_2_9b_it_anfass2_tagliato.txt, /content/frasi_semplificate_gemma_2_9b_it_anfass1_tagliato.txt, frasi_segmentate_per_evaluation_tagliato.txt. These were filtered to exclude sentences modified by the LLM and used in the final evaluation.

Repository Structure
Notebooks/ – Google Colab notebooks for all steps.

Scripts/ – Standalone Python scripts for preprocessing, training, evaluation, and test set extraction.

Data/ – Input data files, including manual files.

Models/ – Saved models (if applicable).

Requirements
Install dependencies using:

bash
Copia
Modifica
pip install -r requirements.txt
