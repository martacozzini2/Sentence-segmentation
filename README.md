# Sentence Segmentation Project

This repository contains scripts and notebooks for sentence segmentation tasks, focusing on training and evaluating a Decision Tree classifier and comparing it with large language models (LLMs).

## Overview

The notebooks are ready to run on Google Colab with Google Drive mounted.

The scripts can be run locally or in any Python environment with the required packages.

### Repository Structure

**Notebooks** – Jupyter notebooks ready to run on Google Colab.

**Scripts** – Python scripts that can be run locally or on any Python environment.

**Data** – Input and output data files. These files are used by both notebooks and scripts.


# Running the notebooks on Google Colab

The notebooks are prepared to be run on Google Colab with Google Drive mounted.

```python
from google.colab import drive
drive.mount('/content/drive') 
```

**Data files:** 
Before running a notebook, upload the required input files (found in the `data/` folder of this repository) into the `/content` directory of your Colab session.
This ensures the notebook can access the data correctly.

**Manual Files** 
Several important files were created manually or semi-manually (i.e., with some automated methods plus manual revision) and are not generated by scripts in this repository. They are necessary for training and evaluation, those files are:

- `output_corpus_no_ricette.txt`: Cleaned version of the Due Parole corpus with recipe sections partially removed, used to build the training DataFrame.
The recipes belonged to the "Vita in casa" section of the corpus, which contained many cooking texts. We decided to exclude parts of these recipes because, after splitting the Due Parole texts into sentences, we noticed that the spaCy sentencizer often failed in the "Che cosa serve" subsections, where ingredients were listed in bullet points.
These passages were frequently segmented incorrectly, mainly due to the presence of bullet-point lists introduced by symbols like hyphens (“-”).
The file `output_corpus_no_ricette.txt` was manually created from `output_corpus_in_frasi.txt`, which is the output of the script `corpus_in_frasi_dueparole.py` (located in the preprocessing/ folder).
Finally, `output_corpus_no_ricette.txt` is used by the script `dataframe_dueparole.py` to build the DataFrame for training the Decision Tree.

- `corpus_in_frasi_anfass_tagliato_perdecision.txt`: Derived corpus excluding sentences modified by the LLM model, to ensure fair evaluation comparability. This txt was manually created from the file `corpus_in_frasi_anfass.txt`.

- The files in the subfolder `modified_outputs` (inside the `output_LLM folder`) are manually created from the files in the subfolder `original_outputs` of the same folder.
They contain the original files with the sentences modified by the LLM removed.


## Running the scripts locally

The Python scripts in this repository can be run in any Python environment.

**Setup Environment (Local Linux)**
If you want to run the scripts locally on a Linux machine, follow these steps:

1. Create and activate a Python virtual environment:
```python
python -m venv env
source env/bin/activate
```
2. Install the required packages:
```python
pip install -r requirements.txt
```
3. Download the spaCy Italian language model:
```python
python -m spacy download it_core_news_sm
```

### Hugging Face Authentication

One script and one notebook located in the LLM folder require access to Hugging Face models.
To run them, you need to authenticate using a Hugging Face token.

You can get your token from: https://huggingface.co/settings/tokens

Create a `.env` file in your project root directory with the following content:

```bash
HF_TOKEN=your_token_here
```

Make sure to replace your_token_here with your actual Hugging Face token.

The scripts use this environment variable to log in automatically.

2. The code uses this token automatically via the huggingface_hub library:

```python
from dotenv import load_dotenv
import os
from huggingface_hub import login

load_dotenv()  # Loads the token from the .env file
hf_token = os.getenv("HF_TOKEN")
login(token=hf_token)
```

Make sure the required packages are installed:

```python
pip install huggingface_hub python-dotenv
```



# How to Run the Project

Running the scripts (local or remote environment)
To reproduce the full pipeline, run the scripts in the following order:

1. **Preprocessing script**
These scripts are stored in the 'preprocessing' folder.
They prepare and clean the corpus files, by dividing the two corpora in sentences and generate the DataFrame used for training the decision tree models.

2. **Decision Tree scripts**
These scripts are stored in the 'decision_tree' folder.
They train and evaluate a Decision Tree classifier on the three different preprocessed DataFrames, which are prepared with the scripts in the preprocessing folder.
The scripts generate metrics and confusion matrices and evaluate the Decision Tree classifier on the Due Parole test set, the Anfass test set, and the Anfass reduced test set, with the outputs modified by the LLM removed, in order to make the results comparable with those of the LLM.

3. **LLM scripts**
Perform the task of sentence segmentation using an LLM, the model Gemma 9 2b it tested on 2 different prompts.

4. **LLM evaluation scripts**
Evaluates the performance of the LLM tested on the Anfass test set. 


